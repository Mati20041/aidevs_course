export interface ModerationResult {
    "id": string,
    "model": string,
    "results":
        {
            "categories": {
                "hate": boolean,
                "hate/threatening": boolean,
                "self-harm": boolean,
                "sexual": boolean,
                "sexual/minors": boolean,
                "violence": boolean,
                "violence/graphic": boolean
            },
            "category_scores": {
                "hate": number,
                "hate/threatening": number,
                "self-harm": number,
                "sexual": number,
                "sexual/minors": number,
                "violence": number,
                "violence/graphic": number
            },
            "flagged": boolean
        }[]
}

export type ChatRole = 'system' | 'user' | 'assistant';

export interface ChatMessage {
    role: ChatRole,
    content: string
    name?: string
}

export interface ChatOptions {
    model?: 'gpt-3.5-turbo' | 'gpt-4'
    /**
     * What sampling temperature to use, between 0 and 2.
     * Higher values like 0.8 will make the output more random,
     * while lower values like 0.2 will make it more focused and deterministic.
     */
    temperature?: number
    /**
     * An alternative to sampling with temperature, called nucleus sampling,
     * where the model considers the results of the tokens with top_p probability mass.
     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     */
    top_p?: number
    /**
     * How many chat completion choices to generate for each input message.
     */
    n?: number
    /**
     * If set, partial message deltas will be sent, like in ChatGPT.
     * Tokens will be sent as data-only server-sent events as they become available,
     * with the stream terminated by a data: [DONE] message.
     * See the OpenAI Cookbook for example code.
     */
    stream?: boolean
    /**
     * Up to 4 sequences where the API will stop generating further tokens.
     */
    stop?: string | [string, string] | [string, string, string] | [string, string, string, string]
    /**
     * The maximum number of tokens to generate in the chat completion.
     */
    max_tokens?: number
    /**
     * Number between -2.0 and 2.0.
     * Positive values penalize new tokens based on whether they appear
     * in the text so far, increasing the model's likelihood to talk about new topics.
     */
    presence_penalty?: number
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens
     * based on their existing frequency in the text so far,
     * decreasing the model's likelihood to repeat the same line verbatim.
     */
    frequency_penalty?: number
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a json object that maps tokens (specified by their token ID in the tokenizer)
     * to an associated bias value from -100 to 100.
     * Mathematically, the bias is added to the logits generated by the model
     * prior to sampling. The exact effect will vary per model,
     * but values between -1 and 1 should decrease
     * or increase likelihood of selection; values like -100 or 100 should result
     * in a ban or exclusive selection of the relevant token.
     */
    logit_bias?: Record<string, number>
    /**
     * A unique identifier representing your end-user,
     * which can help OpenAI to monitor and detect abuse. Learn more.
     */
    user?: string
}

export interface ChatResult {
    id: string;
    object: string;
    created: number;
    model: string;
    usage: {
        "prompt_tokens": number;
        "completion_tokens": number;
        "total_tokens": number;
    }
    "choices":
        {
            "message": Omit<ChatMessage, 'name'>,
            "finish_reason": string,
            "index": number
        }[]

}